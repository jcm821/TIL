{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f0afb4",
   "metadata": {},
   "source": [
    "# 텍스트를 위한 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83ad9c",
   "metadata": {},
   "source": [
    "## 시퀀스 데이터(Sequence data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30cd92",
   "metadata": {},
   "source": [
    "* 순서가 있는 데이터\n",
    "* 특정 순서에 따라 요소들이 배열되어 있다.\n",
    "    - 순서 정보가 데이터의 중요한 특징\n",
    "    - 순서가 변경되면 데이터의 의미나 패턴이 변경될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89657795",
   "metadata": {},
   "source": [
    "### 시퀀스 데이터의 종류\n",
    "* **자연어 텍스트**\n",
    "    - 단어들이 순서대로 나열되어 문장을 형성하며, 문맥과 문법에 따라 의미가 형성된다.\n",
    "    - 단어 시퀀스, 문자 시퀀스, 문장 시퀀스 등 다양하다.\n",
    "* **시계열 데이터**\n",
    "    - 시간 순서에 따라 측정된 값들이 나열됨\n",
    "    - ex) 주식가격, 기온, 판매량,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffb41a",
   "metadata": {},
   "source": [
    "## 텍스트 벡터화(vectorizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e19b0",
   "metadata": {},
   "source": [
    "### 텍스트를 수치형 텐서로 변환하는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94da81",
   "metadata": {},
   "source": [
    "* 원시 텍스트를 벡터로 바꾸기 [ 텍스트 → (표준화) → 표준화된 텍스트 → (토큰화) → 토큰 → (인덱싱) → 토큰 인덱스 → (원-핫 인코딩 or 임베딩) → 인덱스의 벡터 인코딩 ]\n",
    "* 텍스트 표준화: 모델이 인코딩 차이를 고려하지 않도록 제거하기 위한 기초적인 특성 공학의 한 형태(소문자 변환 등)\n",
    "* 토큰(token): 텍스트를 나누는 단어, 문자, 단위\n",
    "* 토큰화(tokenization): 텍스트를 토큰으로 나누는 작업: 단어 수준 토큰화, N-그램 토큰화, 문자 수준 토큰화\n",
    "* 토큰화를 위한 두 종류의 텍스트 처리 모델\n",
    "    - BoW 모델(N-그램 토큰화 사용)\n",
    "    - 시퀀스 모델\n",
    "* 토큰을 수치형 벡터에 연결하는 방법\n",
    "    - 원핫 인코딩(One-hot encoding)\n",
    "    - 임베딩(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f03a07",
   "metadata": {},
   "source": [
    "## TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716dd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# 텍스트 벡터화 클래스 생성\n",
    "class Vectorizer:\n",
    "    \n",
    "    # 텍스트 표준화\n",
    "    def standardize(self, text):\n",
    "        # 텍스트를 소문자로 변환\n",
    "        text = text.lower()\n",
    "        # text의 각 문지 char에 대해 반목하되, char가 string.punctuation(구두점:, 문장부호, 특수기호 등)에 속하지 않는 경우에만 그대로 유지\n",
    "        # join을 이용하여 하나의 문자열로 결합\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        # 공백을 기준으로 분리\n",
    "        return text.split()\n",
    "    \n",
    "    # 토큰에 대한 어휘사전 생성\n",
    "    def make_vocabulary(self, dataset):\n",
    "        # 어휘사전의 첫 두 항목은 공백(0)과 unknown(1)\n",
    "        # self.vocabulary 변수는 텍스트 데이터셋에서 생성된 어휘사전을 저장하는 변수\n",
    "        # 초기값으로는 빈 문자열과(\"\") unknown('[UNK]')을 각각 0과 1의 인덱스로 가짐\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        # dataset 내 문자열에 대하여 반복\n",
    "        for text in dataset:\n",
    "            # 표준화\n",
    "            text = self.standardize(text)\n",
    "            \n",
    "            # 공백 기준 토큰화\n",
    "            tokens = self.tokenize(text)\n",
    "            \n",
    "            # 어휘사전에 token이 없으면 새로운 단어로 판단하여 'self.vocabulary'에 추가\n",
    "            # 'self.vocabulary'의 현재 크기, 즉 추가된 단어의 개수를 이 단어의 인덱스 값으로 할당\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "                    \n",
    "        # 생성된 어휘사전의 key와 value를 바꾼 inverse_vocabulary 생성, 정수 인덱스: 토큰\n",
    "        self.inverse_vocabulary = dict(\n",
    "            # 'self.vocabulary.items()' 메소드를 사용하여 'self.vocabulary' 사전의 모든 (key, value) 쌍을 가져와\n",
    "            # 이를 반대로 뒤집은 새로운 (value, key) 쌍을 생성\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "        \n",
    "        \n",
    "    # 전달된 텍스트를 표준화, 토큰화를 한 후 각 토큰에 대하며 어휘사전에 해당하는 정수 인덱스를 찯아서 대체함\n",
    "    def encode(self, text):\n",
    "        # 표준화\n",
    "        text = self.standardize(text)\n",
    "        # 토큰화\n",
    "        tokens = self.tokenize(text)\n",
    "        # 정수 인덱스를 찾아 대체\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    \n",
    "    # inverse_vocabulary를 사용하여 인코더와 반대로 전달된 정수 시퀀스를 원래 텍스트로 복원\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(self.inverse_vocabulary.get(i, '[UNK]') for i in int_sequence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e31305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 객체 생성\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "# 데이터로 사용할 수 있는 텍스트 변수 정의\n",
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms.',\n",
    "]\n",
    "\n",
    "# make_vocabulary 메소드 실행\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bad48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# 만들어진 어휘사전으로 새로운 텍스트 시퀀스 인코딩\n",
    "test_sentence = 'I write, rewrite, and still rewrite again'\n",
    "# test_sentence를 vectorizer.encode()를 이용하여 정수 시퀀스로 인코딩하여 변수에 저장\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10d0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# test_sentence_1\n",
    "test_sentence_test1 = 'I write, erase, rewrite'\n",
    "encoded_sentence_test1 = vectorizer.encode(test_sentence_test1)\n",
    "print(encoded_sentence_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107ef9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# test_sentence_2\n",
    "test_sentence_test2 = 'Erase again, and then.'\n",
    "encoded_sentence_test2 = vectorizer.encode(test_sentence_test2)\n",
    "print(encoded_sentence_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1e5435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "# test_sentence_3\n",
    "test_sentence_test3 = 'I write, erase and delete'\n",
    "encoded_sentence_test3 = vectorizer.encode(test_sentence_test3)\n",
    "print(encoded_sentence_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "536942a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 7, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# test_sentence_4\n",
    "test_sentence_test4 = 'I like apple, and also banana'\n",
    "encoded_sentence_test4 = vectorizer.encode(test_sentence_test4)\n",
    "print(encoded_sentence_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc809ba",
   "metadata": {},
   "source": [
    "* 본 예제에서는 공백을 기준으로 토큰화를 진행하였으므로, 빈 문자열인 0의 경우는 없게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a0df21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# 위의 인코딩 값을 디코딩(다시 텍스트로 복원)\n",
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82976047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write erase rewrite\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence_test1 = vectorizer.decode(encoded_sentence_test1)\n",
    "print(decoded_sentence_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d503fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erase again and then\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence_test2 = vectorizer.decode(encoded_sentence_test2)\n",
    "print(decoded_sentence_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a3c84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write erase and [UNK]\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence_test3 = vectorizer.decode(encoded_sentence_test3)\n",
    "print(decoded_sentence_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528908cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i [UNK] [UNK] and [UNK] [UNK]\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence_test4 = vectorizer.decode(encoded_sentence_test4)\n",
    "print(decoded_sentence_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1677fa",
   "metadata": {},
   "source": [
    "위의 방식은 성능이 좋지 않다. 따라서 빠르고 효율적인 keras의 TextVectorization을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d9483a",
   "metadata": {},
   "source": [
    "## 케라스에서 제공하는 텍스트 벡터화 레이어 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5227a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "# 객체 생성\n",
    "# 정수 인덱스로 인코딩 되도록 설정\n",
    "text_vectorization = TextVectorization(output_mode = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb0f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식(Regular Expression)을 사용하여 문자열을 처리하는 라이브러리\n",
    "import re\n",
    "# 문자열 관련 유틸리티 함수 제공\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# 텍스트 표준화 사용자 함수 작성\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    # string_tensor 내 모든 문자열을 소문자로 변환\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    \n",
    "    # tf.strings.regex_replace는 input, pattern, rewrite를 인자로 받는다\n",
    "    # re.escape(): 특수문자 등을 escape(삭제)하여 정규표현식에서 사용할 수 있는 형태로 만들어 줌\n",
    "    # f'[{re.escape(string.punctuation)}]': 구두점(string.punctuation)을 찾아 정규표현식 패턴으로 만듦\n",
    "    # 입력문 lowercase_string에서 구두점 문자를 찾아 \"\"로 변환하여 반환\n",
    "    return tf.strings.regex_replace(lowercase_string, f'[{re.escape(string.punctuation)}]', \"\")\n",
    "\n",
    "# 문자열을 공백 기준으로 분할\n",
    "def custom_split_fn(string_tensor):\n",
    "    # 문자열을 분리\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "# 텍스트 벡터화 레이어 생성\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode = 'int',      # 출력결과의 형태\n",
    "    standardize = custom_standardization_fn,       # 텍스트 표준화 함수 설정: 위의 사용자 함수를 사용\n",
    "    split = custom_split_fn,        # 토큰화 함수 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c6c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.strings.regex_replace() 더 살펴보기\n",
    "test_text = \"GOOD~GOOD~GOOD~GOOD!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca8a5e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\~'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.escape('~')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d8311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'GOODGOODGOODGOOD!!'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_2 = tf.strings.regex_replace(test_text, m, \"\")\n",
    "test_text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc88ac8",
   "metadata": {},
   "source": [
    "* 텍스트 말뭉치의 어휘 사전을 인덱싱하기 위해 문자열을 반환하는 Dataset 객체로 이 층의 adapt()메서드를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe5099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data 정의\n",
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms.',\n",
    "]\n",
    "# TextVectorization 클래스 객체가 현재 데이터셋에 맞게 적용(adapt)되어, 어휘 사전 생성\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88f283",
   "metadata": {},
   "source": [
    "### 어휘 사전 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f366ccca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어휘 사전 출력(0은 공백, 1은 unknown)\n",
    "# TextVectorization 클래스를 사용하여 생성된 어휘 사전(vocabulary)을 조회하는 메소드\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9947bb",
   "metadata": {},
   "source": [
    "* 앞선 클래스 정의때와는 달리, 케라스의 텍스트 벡터화 레이어의 어휘사전은 가장 빈도가 많은 단어, 알파벳의 역순으로 정렬이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8243fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 인코딩\n",
    "\n",
    "# 어휘 사전을 vocabulary 변수에 할당\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "\n",
    "# 새로운 input data 정의\n",
    "test_sentence = 'I write, rewrite, and still rewrite again'\n",
    "\n",
    "# 벡터화 레이어에 새로운 input data를 투입하여 인코딩된 정수 시퀀스 반환\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "256a66dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# 디코딩\n",
    "# inverse_vocabulary 생성\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "\n",
    "# enumerate()는 리스트를 입력값으로 받아서 인덱스, 값을 쌍으로 반환(인덱스는 0부터 시작한다)\n",
    "# dict()는 key-value 쌍을 입력으로 받아서, 딕셔너리 생성\n",
    "# dict(enumerate(vocabulary)): 각 단어에 대한 인덱스를 키(key)로,  각 단어를 값(value)으로 저장하므로\n",
    "# 이 딕셔너리를 어휘 사전의 inverse(역사전)으로 사용\n",
    "\n",
    "# 정수 시퀀스를 다시 텍스트화\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb563b",
   "metadata": {},
   "source": [
    "# 토큰화를 위한 두 종류의 텍스트 처리 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df91528",
   "metadata": {},
   "source": [
    "## IMDB 영화 리뷰 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2c4701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 80.2M    0 32768    0     0  31256      0  0:44:51  0:00:01  0:44:50 31297\n",
      "  0 80.2M    0  368k    0     0   177k      0  0:07:42  0:00:02  0:07:40  177k\n",
      "  1 80.2M    1  896k    0     0   297k      0  0:04:36  0:00:03  0:04:33  297k\n",
      "  1 80.2M    1 1616k    0     0   401k      0  0:03:24  0:00:04  0:03:20  401k\n",
      "  3 80.2M    3 2768k    0     0   552k      0  0:02:28  0:00:05  0:02:23  553k\n",
      "  5 80.2M    5 4624k    0     0   763k      0  0:01:47  0:00:06  0:01:41  917k\n",
      "  8 80.2M    8 7216k    0     0  1030k      0  0:01:19  0:00:07  0:01:12 1387k\n",
      " 13 80.2M   13 11.1M    0     0  1420k      0  0:00:57  0:00:08  0:00:49 2098k\n",
      " 21 80.2M   21 17.4M    0     0  1983k      0  0:00:41  0:00:09  0:00:32 3261k\n",
      " 30 80.2M   30 24.5M    0     0  2483k      0  0:00:33  0:00:10  0:00:23 4381k\n",
      " 40 80.2M   40 32.7M    0     0  3048k      0  0:00:26  0:00:11  0:00:15 5845k\n",
      " 50 80.2M   50 40.6M    0     0  3465k      0  0:00:23  0:00:12  0:00:11 6878k\n",
      " 62 80.2M   62 49.9M    0     0  3837k      0  0:00:21  0:00:13  0:00:08 7475k\n",
      " 68 80.2M   68 54.7M    0     0  4004k      0  0:00:20  0:00:14  0:00:06 7644k\n",
      " 74 80.2M   74 60.0M    0     0  4098k      0  0:00:20  0:00:15  0:00:05 7437k\n",
      " 81 80.2M   81 65.6M    0     0  4197k      0  0:00:19  0:00:16  0:00:03 6724k\n",
      " 89 80.2M   89 71.4M    0     0  4299k      0  0:00:19  0:00:17  0:00:02 6304k\n",
      " 96 80.2M   96 77.4M    0     0  4403k      0  0:00:18  0:00:18 --:--:-- 6016k\n",
      "100 80.2M  100 80.2M    0     0  4449k      0  0:00:18  0:00:18 --:--:-- 5846k\n"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 압축 해제\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "# unsup 폴더 삭제\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "950bea66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\n"
     ]
    }
   ],
   "source": [
    "# 데이터 샘플 확인\n",
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "557be13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_dir = pathlib.Path('aclImdb')\n",
    "\n",
    "# 검증 데이터 경로 설정\n",
    "val_dir = base_dir / 'val'\n",
    "\n",
    "# 학습 데이터 경로 설정\n",
    "train_dir = base_dir / 'train'\n",
    "\n",
    "# 'neg'와 'pos' 두 카테고리에 대해 반복문을 실행\n",
    "# 'neg': 부정적 리뷰, 'pos': 긍정적 리뷰\n",
    "for category in ('neg', 'pos'):\n",
    "    # 각 카테고리에 대한 검증 데이터셋 디렉토리를 생성\n",
    "    os.makedirs(val_dir / category)\n",
    "    \n",
    "    # 현재 카테고리의 훈련 데이터셋 디렉토리에서 모든 파일 목록을 가져와 files 변수에 저장\n",
    "    files = os.listdir(train_dir / category)\n",
    "    \n",
    "    # 시드값 1337으로 files 변수에 저장된 파일들을 무작위로 섞음\n",
    "    random.Random(1337).shuffle(files)\n",
    "    \n",
    "    # 훈련 데이터셋에서 20%를 검증 데아터로 사용\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    \n",
    "    # 추출된 검증 데이터 파일들을 순회\n",
    "    for fname in val_files:\n",
    "        # 각 파일을 훈련 데이터셋 디렉토리에서 검증 데이터셋 디렉토리로 이동\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d53b5c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# 배치 사이즈를 32로 설정\n",
    "batch_size = 32\n",
    "\n",
    "# 학습 데이터셋 로드\n",
    "train_ds = keras.utils.text_dataset_from_directory('aclImdb/train', batch_size = batch_size)\n",
    "\n",
    "# 검증 데이터셋 로드\n",
    "val_ds = keras.utils.text_dataset_from_directory('aclImdb/val', batch_size = batch_size)\n",
    "\n",
    "# 평가 데이터셋 로드\n",
    "test_ds = keras.utils.text_dataset_from_directory('aclImdb/test', batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a87d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (32,)\n",
      "inputs.dtype:  <dtype: 'string'>\n",
      "targets.shape:  (32,)\n",
      "targets.dtype:  <dtype: 'int32'>\n",
      "inputs[0]:  tf.Tensor(b\"It Could Have Been A Marvelous Story Based On The Ancient Races Of Cat People, but it wasn't.<br /><br />This work could have been just that; marvelous and replete with mythological references which kept my fascination fueled. The lead characters (Charles Brady played by Brian Krause; and his mother Mary, played by Alice Krige) were shallowly done, had no depth of personality and were hardly likable or drawing. Not even M\\xc3\\xa4dchen Amick (who played Tanya Robertson)'s character fit into that description. <br /><br />However, as I've said many times before, when you adapt a Stephen King novel for TV, you simply must take into account the fact that his books aren't written for TV, and his screenplay talent sadly lacks the fire and depth he exhibits as a novelist. <br /><br />This is another botched attempt to take the magick of Stephen King writing, whether that is of his novels or an original screenplay. To simply cut and paste his work onto the small screen. His novels get completely bastardized in the process and all you end up creating is a nice movie; nothing less but certainly nothing more. His screenplays are hit and miss. Unfortunately, this screenplay translation was a miss. <br /><br />Sorry, Sorry, Sorry movie.<br /><br />This movie gets a 1.0/10 from...<br /><br />the Fiend :.\", shape=(), dtype=string)\n",
      "targets[0]:  tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 배치의 크기와 dtype 출력하기\n",
    "for inputs, targets in train_ds:\n",
    "    # 입력 데이터의 형태\n",
    "    print('inputs.shape: ', inputs.shape)\n",
    "    # 입력 데이터의 데이터 타입\n",
    "    print('inputs.dtype: ', inputs.dtype)\n",
    "    # 타겟 데이터의 형태\n",
    "    print('targets.shape: ', targets.shape)\n",
    "    # 타겟 데이터의 데이터 타입\n",
    "    print('targets.dtype: ', targets.dtype)\n",
    "    # 입력 데이터의 첫 번째 샘플 출력\n",
    "    print('inputs[0]: ', inputs[0])\n",
    "    # 타겟 데이터의 첫 번째 샘플 출력\n",
    "    print('targets[0]: ', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986caa8",
   "metadata": {},
   "source": [
    "## 단어를 집합으로 처리하기: BoW(Bag of Word)방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef2a63",
   "metadata": {},
   "source": [
    "* Text: 'the cat sat on the mat'\n",
    "    - 1-gram(unigram): ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "    - 2-gram(bi-grams): ['the cat', 'cat sat', 'sat on', 'on the', 'the mat']\n",
    "    - 3-gram(tri-grams): ['the cat sat', 'cat sat on', 'sat on the', 'on the mat']\n",
    "    - 4-gram ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ffd12",
   "metadata": {},
   "source": [
    "* 이런 종류의 단어 집합을 사용한 토큰화 방법을 Bag of Word라고 한다.\n",
    "    - 생성된 토큰은 시퀀스가 아니라 집합이며, 문장의 일반적인 구조가 사라진다.\n",
    "    - 단순한 집합을 담은 가방을 사용한다고 하여 Bag of Word이라는 용어로 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2ecd5",
   "metadata": {},
   "source": [
    "### Single words (unigrams) with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cb240",
   "metadata": {},
   "source": [
    "### TextVectorization 층으로 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe2255a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "# 텍스트 벡터화 레이어 생성\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens = 20000,      # 토큰 최대값(문장 내의 최대 단어 수, 입력값의 길이를 제한)\n",
    "    output_mode = 'multi_hot',    # 단어의 개수만큼 차원을 가진 벡터로 인코딩\n",
    ")\n",
    "\n",
    "# 학습 데이터셋(train_ds)에서 텍스트 데이터만 추출하여 text_only_train_ds에 저장\n",
    "# train_ds 데이터셋에 대해 map함수를 적용하여 모든 샘플에서 x값만 추출\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# text_omly_train_ds로 어휘사전 생성\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# 학습데이터 셋의 텍스트 데이터에 어휘사전을 적용한 후 저장\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls = 4)\n",
    "# 검증데이터 셋의 텍스트 데이터에 어휘사전을 적용한 후 저장\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls = 4)\n",
    "# 퍙가데이터 셋의 텍스트 데이터에 어휘사전을 적용한 후 저장\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1c76b",
   "metadata": {},
   "source": [
    "### 모델 생성 사용자 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c3d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "# 토큰 최대값, 은닉차원의 수는 16로 설정\n",
    "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
    "    \n",
    "    # 함수형 API를 사용한 레이어 생성\n",
    "    # 입력 텐서 정의\n",
    "    inputs = keras.Input(shape = (max_tokens, ))\n",
    "    # 은닉 레이어\n",
    "    x = layers.Dense(hidden_dim, activation = 'relu')(inputs)\n",
    "    # 드롭아웃 레이어\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    # 출력 레이어\n",
    "    outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    # 모델 생성\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # 모델 캄파일\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07136afa",
   "metadata": {},
   "source": [
    "### 이진 유니그램 데이터 학습하고 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "894abfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 13s 19ms/step - loss: 0.3973 - accuracy: 0.8321 - val_loss: 0.3001 - val_accuracy: 0.8794\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2682 - accuracy: 0.9022 - val_loss: 0.3194 - val_accuracy: 0.8808\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2412 - accuracy: 0.9140 - val_loss: 0.3269 - val_accuracy: 0.8826\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2249 - accuracy: 0.9240 - val_loss: 0.3420 - val_accuracy: 0.8816\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2182 - accuracy: 0.9301 - val_loss: 0.3563 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2142 - accuracy: 0.9304 - val_loss: 0.3708 - val_accuracy: 0.8792\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2075 - accuracy: 0.9326 - val_loss: 0.3876 - val_accuracy: 0.8786\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2135 - accuracy: 0.9342 - val_loss: 0.3951 - val_accuracy: 0.8724\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1975 - accuracy: 0.9363 - val_loss: 0.4095 - val_accuracy: 0.8738\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2000 - accuracy: 0.9374 - val_loss: 0.4282 - val_accuracy: 0.8782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16330e868e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = get_model()\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()\n",
    "\n",
    "# 사용자 콜백 생성\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only = True)\n",
    "]\n",
    "\n",
    "# 모델 학습\n",
    "# cache(): 데이터셋을 메모리에 캐시하여 데이터 처리 과정에서 발생하는 오버헤드 감소(작은 데이터에서 사용함)\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "         validation_data = binary_1gram_val_ds.cache(), epochs = 10, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0a7c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백에서 저장한 best 모델 로드\n",
    "model = keras.models.load_model('binary_1gram.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f81060a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 26s 32ms/step - loss: 0.2907 - accuracy: 0.8864\n",
      "테스트 정확도: 0.886\n"
     ]
    }
   ],
   "source": [
    "# 평가 지표 출력\n",
    "print(f'테스트 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1dc0a",
   "metadata": {},
   "source": [
    "### Bi-grams with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb0e09",
   "metadata": {},
   "source": [
    "### 바이그램을 반환하는 TextVectorization 층 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9227981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 벡터화 레이어 생성\n",
    "# 바이그램이므로 ngrams는 2로 설정\n",
    "text_vectorization = TextVectorization(ngrams = 2, max_tokens = 20000, output_mode = 'multi_hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71fdb69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3734 - accuracy: 0.8463 - val_loss: 0.2789 - val_accuracy: 0.8888\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2335 - accuracy: 0.9179 - val_loss: 0.2815 - val_accuracy: 0.8940\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1979 - accuracy: 0.9347 - val_loss: 0.3072 - val_accuracy: 0.8914\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1739 - accuracy: 0.9438 - val_loss: 0.3274 - val_accuracy: 0.8952\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1652 - accuracy: 0.9503 - val_loss: 0.3384 - val_accuracy: 0.8912\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1659 - accuracy: 0.9519 - val_loss: 0.3734 - val_accuracy: 0.8906\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1677 - accuracy: 0.9552 - val_loss: 0.3821 - val_accuracy: 0.8876\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1579 - accuracy: 0.9553 - val_loss: 0.3780 - val_accuracy: 0.8864\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1507 - accuracy: 0.9582 - val_loss: 0.3994 - val_accuracy: 0.8894\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1544 - accuracy: 0.9596 - val_loss: 0.4005 - val_accuracy: 0.8812\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.2651 - accuracy: 0.8990\n",
      "테스트 정확도: 0.899\n"
     ]
    }
   ],
   "source": [
    "# 어휘 사전 생성\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# 학습데이터 셋의 텍스트 데이터에 어휘사전을 적용한 후 저장\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls = 4)\n",
    "# 검증데이터 셋의 텍스트 데이터에 어휘사전을 적용한 후 저장\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls = 4)\n",
    "# 평가데이터 셋의 텍스트 데이터에 어휘사전을 적용한 후 저장\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls = 4)\n",
    "\n",
    "# 모델 생성\n",
    "model = get_model()\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()\n",
    "\n",
    "# 사용자 콜백 작성\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_2gram.keras', save_best_only = True)\n",
    "]\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(binary_2gram_train_ds.cache(), validation_data = binary_2gram_val_ds.cache(), epochs = 10, callbacks = callbacks)\n",
    "\n",
    "# best 모델 로드\n",
    "model = keras.models.load_model('binary_2gram.keras')\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(f'테스트 정확도: {model.evaluate(binary_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249ba46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
